\documentclass[cheatsheet.tex]{subfiles}
\begin{document}
Similarity is some function of distance. Clustering is grouping data without prior information(unlabeled data). \textbf{Why cluster?} \textbullet make apparent the natural groupings/structure in the data (perhaps for further processing)
\textbullet To discover previously unknown relationships \textbullet To provide generic labels for the data
\\
\textbf{the basic linear classifier} can be interpreted from a distance-based perspective as constructing exemplars that minimise squared Euclidean distance within each class, and then applying a nearest-exemplar decision rule
\\
\textbf{Clustering} we organize data into classes such that:
\textbullet The within-class (intra-class) similarity is high(Lower intra-class variance)
\textbullet The between-class (inter-class) similarity is low(Higher inter-class variance)
\textbullet Objects in the same group (a cluster) are more similar to one another than to objects in other groups (clusters)
\\
\textbf{Distance measures} D(x1,x2) is a function D: $X\times X\rightarrow R$ such that for any xyz in X:
1. D(x,x) = 0  2. If x != y then $D(x,y) > 0$ 3. D(x,y) = D(y,x) 4. $D(x,z)\leq D(x,y)+D(y,z)$. \textbf{Common Distance measures}Hamming(count differences), Manhattan, Euclidian, Minkowski($L_p$), Chebyshev, Mahalanobis
\\
\textbf{Distance-based methods} Methods for classification and clustering based on distances to exemplars or neighbors. Exemplar - a prototypical instance. Neighbor -- a nearby instance or exemplar. 
\\
\textbf{1NN} Assign the new instance x to the nearest labeled training point (or exemplar). 
\textbullet Training = memorizing the training data
\textbullet Each point is an exemplar, or exemplars are computed from the data
\textbullet But it generalizes, unlike the lookup table approach
\textbullet The implicit decision boundaries of a 1NN classifier comprise a \textbf{Voronoi tesselation}, Leads to piecewise linear decision boundaries.
\end{document}
