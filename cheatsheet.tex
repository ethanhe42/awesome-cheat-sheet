\documentclass[8pt]{extarticle} % extarticle: font sizes < 10
\usepackage[
      pdftitle={Modern Fortran Reference Card},
      pdfauthor={Michael Goerz},
      pdfkeywords={Fortran 90, Fortran 95, Fortran 2003, Fortran 2008, Quick Reference, Refcard, Cheat Sheet},
      pdfsubject={Quick Reference Card for Fortran 90/95/2003/2008}
]{hyperref}
\usepackage{refcards}
\usepackage{vmargin}
\usepackage{lipsum} 

% A4
\setpapersize[landscape]{A4}
\setmarginsrb%
{0.3cm}  % left
{0.3cm}  % top
{0.3cm}  % right
{0.3cm}  % bottom
{0pt}    % header height
{0mm}    % header separation
{0pt}    % footer height
{0mm}    % footser separation
\setlength\columnsep{1mm}

% Letter
%\setpapersize[landscape]{USletter}
%\setmarginsrb%
%{1.1cm}  % left
%{1.1cm}  % top
%{0.9cm}  % right
%{0.9cm}  % bottom
%{0ex}    % header height
%{0ex}    % header separation
%{0ex}    % footer height
%{0ex}    % footser separation
%\setlength\columnsep{4mm}
\begin{document}
\raggedright
\begin{multicols}{3}


\section{Key ingredients}
\lipsum[2-4]
\section{Classification}
\lipsum[2-4]
\section{Concept learning}
\lipsum[2-4]
\section{Decision trees}
\lipsum[2-4]
\section{Linear models}
\lipsum[2-4]
\section{Distance-based models}
\lipsum[1-4]
\section{Probabilistic models}
\lipsum[1-4]
\section{Features}
\lipsum[1-4]
\section{Model ensembles}
\lipsum[1-4]
\section{ML experiments}
\lipsum[1-4]
\section{Neural network}
\textbf{RNN} Directed cycles exist.\\

\textbf{GD} search \textit{hypothesis space} to find $w$ for minimum error(slow, local minimum). \textit{Variation} incremental GD, SGD.\\

\textbf{BP} minimize the squared error, iteratively propagate errors backwards from output units.$\Delta w_{ji}=\eta \delta_jz_{ji}$. \textbf{inductive bias}  smooth interpolation between data points. \textbf{termination condition} number of iterations, predetermined error threshold. \textbf{Overfitting} every iteration measure error on the validation set(cross-validation approach).\\

\textbf{RL} agent learns behavior through trial-and-error interactions with a dynamic environment based on a reward signal(Game playing). \textbf{Rewards} from sequences of actions, can be separated temporally. \textbf{Q-learning} finds optimal action-selection policy for Markov Decision Process, learns an action-value function that gives the expected utility(convergence slow).\\

\textbf{Multi-label classification} class labels are not mutually exclusive.feature-to-class mapping, dependence between classes are learned.(blog post tags)\\

\textbf{Transfer learning}(inductive transfer) apply knowledge to a different but related problem(walk to run)\\

\textbf{Online (incremental) learning} data available sequentially over time. updates the model each time a new data point arrives(visual tracking)\\

\textbf{Active learning} semi-supervised learning, the algorithm queries information source(the user) to obtain the desired model(robotics, where to point the camera or sensors to gain useful information)\\

\textbf{DL} learn many-layered NN, more abstract features. learn good representations of the data through unsupervised learning. \textbf{Cons} complex, slow, hard to explain.

\end{multicols}
\end{document}