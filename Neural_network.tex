\documentclass[cheatsheet.tex]{subfiles}
\begin{document}
\textbf{When to use} input high dim. target function unknown. long training time. human readability. good for complex pattern recognition.\\

\textbf{RNN} Directed cycles exist.\\

\textbf{GD} search \textit{hypothesis space} to find $w$ for minimum error(slow, local minimum). \textit{Variation} incremental GD, SGD.\\

\textbf{BP} minimize the squared error, iteratively propagate errors backwards from output units.$\Delta w_{ji}=\eta \delta_jz_{ji}$. \textbf{inductive bias}  smooth interpolation between data points. \textbf{termination condition} number of iterations, predetermined error threshold. \textbf{Overfitting} every iteration measure error on the validation set(cross-validation approach).\\

\textbf{RL}(Reinforce learning) agent learns behavior through trial-and-error interactions with a dynamic environment based on a reward signal(Game playing). \textbf{Rewards} from sequences of actions, can be separated temporally. \textbf{Q-learning} finds optimal action-selection policy for Markov Decision Process, learns an action-value function that gives the expected utility(convergence slow).\\

\textbf{Multi-label classification} class labels are not mutually exclusive.feature-to-class mapping, dependence between classes are learned.(blog post tags)\\

\textbf{Transfer learning}(inductive transfer) apply knowledge to a different but related problem(walk to run)\\

\textbf{Online (incremental) learning} data available sequentially over time. updates the model each time a new data point arrives(visual tracking)\\

\textbf{Active learning} semi-supervised learning, the algorithm queries information source(the user) to obtain the desired model(robotics, where to point the camera or sensors to gain useful information)\\

\textbf{DL} learn many-layered NN, more abstract features. learn good representations of the data through unsupervised learning. \textbf{Cons} complex, slow, hard to explain.
\end{document}
