\documentclass[cheatsheet.tex]{subfiles}
\begin{document}
are  “meta” methods. Construct different models from adapted versions of the training data, Combine the predictions by averaging voting, weighted voting. a strong classifier from a set of weak classifiers. \textbf{Pros} improve performance, avoid over-fitting. 
\\
\textbf{Bagging} bootstrap aggregation. T models on different samples. Samples are training data with replacement choosen with uniform probability, called bootstrap sample. Decision boundary is \textit{piecewise linear}.(useful for tree model). \textbf{Subspace sampling} Build each decision tree from a different random subset of the features. $Bagging + subspace sampling = random forests method$. 
\\
\textbf{Boosting} create diverse training sets, add classifiers that do better on the misclassifications from earlier classifiers, by giving them a higher weight(less susceptible to overfitting).Can be extended to multi-class classification. \textbf{Adaboost}(adaptive boosting), As long as the performance of each classifier is better than chance$\epsilon<0.5$ , it is guaranteed to converge to a better classifier.\textbf{Procedure}1. Train a classifier assign it a confidence factor based on the error rate 2. Give misclassified instances a higher weight. 3. Repeat for T classifiers 4. The ensemble predictor is a weighted average of the models (rather than majority vote) 5. Threshold for binary output.
\\
\textbf{Boosting1 vs bagging2} \textbullet{1} Can achieve zero training error by focusing on the misclassifications. A bias reduction technique (increase accuracy) \textbullet{2} With relatively large bootstrap sample sets, there tends to be little diversity in the learned models. A variance-reduction technique (increase consistency).
\\

\end{document}
