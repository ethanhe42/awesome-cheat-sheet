\documentclass[cheatsheet.tex]{subfiles}
\begin{document}
are geometric models for which the regression functions or decision boundaries are linear. (Lines, planes, hyperplanes,N-dimensional planes). An affine function is a linear function plus a constant. 
\textbf{Pros}, Many functions can be reasonably approximated as linear, or at least as piecewise linear. They're simple, and thus easy to train. The math is tractable. They avoid over-fitting, they generalize well when the data is noisy. 
\textbf{Cons} prone to under-fitting, over-simplifying complicated function. 
\textbf{low variance} stability, robustness, Performance on, different testing sets, should be similar. \textbf{High bias} limited accuracy, underfitting, systematic (but consistent) errors. 
\textbf{parametric models} we just need to learn the model parameters. kNN is non-parametric model. 
\subsection{least-square}
Regression learns a function (the regressor) that is a mapping $\hat{f}:X\rightarrow \mathbb{R}$, from examples($x_i, f(x_i)$), Linear regression â€“ the function is linear. The difference between f and $\hat{f}$ is known as the residual $\epsilon=f(x)-\hat(f)(x)$ The least squares method minimizes the sum of the squared residuals. Univariate -- one input variable. Multivariate -- multiple input variables. The slope ($\hat{b}$) is the \textbf{regression coefficient}, $\hat{b}=\sigma_{xy}/\sigma_x^2$
\subsection{perceptron}
\lipsum[1-2]
\subsection{svm}
\lipsum[1-2]
\subsection{non-linear \& probabilities}
\lipsum[1-2]
\end{document}
